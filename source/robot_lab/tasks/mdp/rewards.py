import torch
from torch import distributions
from isaaclab.envs import ManagerBasedRLEnv
from isaaclab.managers import SceneEntityCfg, ManagerTermBase, RewardTermCfg
from isaaclab.sensors import ContactSensor, RayCaster
from isaaclab.assets import Articulation, RigidObject
import isaaclab.utils.math as math_utils

def track_foot_trajectory(
        env: ManagerBasedRLEnv,
        asset_cfg: SceneEntityCfg,
        step_height,
        step_length,
        T,
        foot_offset,
        foot_centre_pos
) -> torch.Tensor:
    
    # Get the actual foot position.

    asset: Articulation = env.scene[asset_cfg.name]

    body_pos_w = asset.data.body_pos_w[:, asset_cfg.body_ids[0], :]
    body_quat_w = asset.data.body_quat_w[:, asset_cfg.body_ids[0], :]

    offset_local = torch.tensor(foot_offset, device=env.device)
    offset_batch = offset_local.repeat(env.num_envs, 1)
    offset_world =  math_utils.quat_apply(body_quat_w, offset_batch)

    feet_pos_w = body_pos_w + offset_world

    # Get the target foot position.

    Ly = step_length / 2.0

    current_time = env.episode_length_buf * env.step_dt
    freq = 1.0 / T
    phase = (current_time * freq) % 1.0

    target_x_rel = torch.zeros_like(phase)
    target_y_rel = torch.zeros_like(phase)
    target_z_rel = torch.zeros_like(phase)

    is_stance = phase < 0.5
    is_swing = ~is_stance

    # Stance
    t_s = phase[is_stance] / 0.5
    target_x_rel[is_stance] = 0.0
    target_y_rel[is_stance] = Ly * (1 - 2 * t_s)
    target_z_rel[is_stance] = 0.0

    # Swing
    t_w = (phase[is_swing] - 0.5) / 0.5
    target_x_rel[is_swing] = 0.0
    target_y_rel[is_swing] = -Ly + (2 * Ly  * t_w)
    target_z_rel[is_swing] = torch.where(t_w < 0.5, step_height * (t_w * 2), step_height * (2 - t_w * 2))

    center_pos = torch.tensor(foot_centre_pos, device=env.device)
    target_pos_w = center_pos.repeat(env.num_envs, 1)

    target_pos_w[:, 0] += target_x_rel
    target_pos_w[:, 1] += target_y_rel
    target_pos_w[:, 2] += target_z_rel

    error = torch.norm(target_pos_w - feet_pos_w, dim=-1)

    print(f"DEBUG [Env 0] Phase: {phase[0].item():.2f} | "
            f"Error: {error[0].item():.3f} | "
            f"Tgt X: {target_pos_w[0, 0].item():.3f} | "
            f"Real X: {feet_pos_w[0, 0].item():.3f} | "
            f"Tgt Y: {target_pos_w[0, 1].item():.3f} | "
            f"Real Y: {feet_pos_w[0, 1].item():.3f} | "
            f"Tgt Z: {target_pos_w[0, 2].item():.3f} | "
            f"Real Z: {feet_pos_w[0, 2].item():.3f}")
        
    return error

def air_time_reward(
        env: ManagerBasedRLEnv,
        left_sensor_cfg: SceneEntityCfg,
        right_sensor_cfg: SceneEntityCfg,
) -> torch.Tensor:
    left_contact_sensor: ContactSensor = env.scene.sensors[left_sensor_cfg.name]
    right_contact_sensor: ContactSensor = env.scene.sensors[right_sensor_cfg.name]
    # Note: We don't want to use last air time since it will keep rewarding the robot even if it only takes one step!!!
    # 1) Get the current air time for the left calf.
    left_current_air_time = left_contact_sensor.data.current_air_time
    # 2) Get the current air time for the right calf.
    right_current_air_time = right_contact_sensor.data.current_air_time
    # 3) Sum the left and right air times.
    summed_air_times = left_current_air_time + right_current_air_time
    summed_air_times = summed_air_times.squeeze(-1)
    # 4) Return the summed value (in seconds), which will then be scaled by the RewardsTerm API separately.
    return summed_air_times

def joint_powers_l1(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """Penalize joint powers on the articulation using L1-kernel"""

    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    return torch.sum(torch.abs(torch.mul(asset.data.applied_torque, asset.data.joint_vel)), dim=1)


class GaitReward(ManagerTermBase):
    def __init__(self, cfg: RewardTermCfg, env: ManagerBasedRLEnv):
        """Initialize the term.

        Args:
            cfg: The configuration of the reward.
            env: The RL environment instance.
        """
        super().__init__(cfg, env)

        self.sensor_cfg = cfg.params["sensor_cfg"]
        self.asset_cfg = cfg.params["asset_cfg"]

        # extract the used quantities (to enable type-hinting)
        self.contact_sensor: ContactSensor = env.scene.sensors[self.sensor_cfg.name]
        self.asset: Articulation = env.scene[self.asset_cfg.name]

        # Store configuration parameters
        self.force_scale = float(cfg.params["tracking_contacts_shaped_force"])
        self.vel_scale = float(cfg.params["tracking_contacts_shaped_vel"])
        self.force_sigma = cfg.params["gait_force_sigma"]
        self.vel_sigma = cfg.params["gait_vel_sigma"]
        self.kappa_gait_probs = cfg.params["kappa_gait_probs"]
        self.command_name = cfg.params["command_name"]
        self.dt = env.step_dt

    def __call__(
        self,
        env: ManagerBasedRLEnv,
        tracking_contacts_shaped_force,
        tracking_contacts_shaped_vel,
        gait_force_sigma,
        gait_vel_sigma,
        kappa_gait_probs,
        command_name,
        sensor_cfg,
        asset_cfg,
    ) -> torch.Tensor:
        """Compute the reward.

        The reward combines force-based and velocity-based terms to encourage desired gait patterns.

        Args:
            env: The RL environment instance.

        Returns:
            The reward value.
        """

        gait_params = env.command_manager.get_command(self.command_name)

        # Update contact targets
        desired_contact_states = self.compute_contact_targets(gait_params)

        # Force-based reward
        foot_forces = torch.norm(self.contact_sensor.data.net_forces_w[:, self.sensor_cfg.body_ids], dim=-1)
        force_reward = self._compute_force_reward(foot_forces, desired_contact_states)

        # Velocity-based reward
        foot_velocities = torch.norm(self.asset.data.body_lin_vel_w[:, self.asset_cfg.body_ids], dim=-1)
        velocity_reward = self._compute_velocity_reward(foot_velocities, desired_contact_states)

        # Combine rewards
        total_reward = force_reward + velocity_reward
        return total_reward

    def compute_contact_targets(self, gait_params):
        """Calculate desired contact states for the current timestep."""
        frequencies = gait_params[:, 0]
        offsets = gait_params[:, 1]
        durations = torch.cat(
            [
                gait_params[:, 2].view(self.num_envs, 1),
                gait_params[:, 2].view(self.num_envs, 1),
            ],
            dim=1,
        )

        assert torch.all(frequencies > 0), "Frequencies must be positive"
        assert torch.all((offsets >= 0) & (offsets <= 1)), "Offsets must be between 0 and 1"
        assert torch.all((durations > 0) & (durations < 1)), "Durations must be between 0 and 1"

        gait_indices = torch.remainder(self._env.episode_length_buf * self.dt * frequencies, 1.0)

        # Calculate foot indices
        foot_indices = torch.remainder(
            torch.cat(
                [gait_indices.view(self.num_envs, 1), (gait_indices + offsets + 1).view(self.num_envs, 1)],
                dim=1,
            ),
            1.0,
        )

        # Determine stance and swing phases
        stance_idxs = foot_indices < durations
        swing_idxs = foot_indices > durations

        # Adjust foot indices based on phase
        foot_indices[stance_idxs] = torch.remainder(foot_indices[stance_idxs], 1) * (0.5 / durations[stance_idxs])
        foot_indices[swing_idxs] = 0.5 + (torch.remainder(foot_indices[swing_idxs], 1) - durations[swing_idxs]) * (
            0.5 / (1 - durations[swing_idxs])
        )

        # Calculate desired contact states using von mises distribution
        smoothing_cdf_start = distributions.normal.Normal(0, self.kappa_gait_probs).cdf
        desired_contact_states = smoothing_cdf_start(foot_indices) * (
            1 - smoothing_cdf_start(foot_indices - 0.5)
        ) + smoothing_cdf_start(foot_indices - 1) * (1 - smoothing_cdf_start(foot_indices - 1.5))

        return desired_contact_states

    def _compute_force_reward(self, forces: torch.Tensor, desired_contacts: torch.Tensor) -> torch.Tensor:
        """Compute force-based reward component."""
        reward = torch.zeros_like(forces[:, 0])
        if self.force_scale < 0:  # Negative scale means penalize unwanted contact
            for i in range(forces.shape[1]):
                reward += (1 - desired_contacts[:, i]) * (1 - torch.exp(-forces[:, i] ** 2 / self.force_sigma))
        else:  # Positive scale means reward desired contact
            for i in range(forces.shape[1]):
                reward += (1 - desired_contacts[:, i]) * torch.exp(-forces[:, i] ** 2 / self.force_sigma)

        return (reward / forces.shape[1]) * self.force_scale

    def _compute_velocity_reward(self, velocities: torch.Tensor, desired_contacts: torch.Tensor) -> torch.Tensor:
        """Compute velocity-based reward component."""
        reward = torch.zeros_like(velocities[:, 0])
        if self.vel_scale < 0:  # Negative scale means penalize movement during contact
            for i in range(velocities.shape[1]):
                reward += desired_contacts[:, i] * (1 - torch.exp(-velocities[:, i] ** 2 / self.vel_sigma))
        else:  # Positive scale means reward movement during swing
            for i in range(velocities.shape[1]):
                reward += desired_contacts[:, i] * torch.exp(-velocities[:, i] ** 2 / self.vel_sigma)

        return (reward / velocities.shape[1]) * self.vel_scale
    
def stay_alive(env: ManagerBasedRLEnv) -> torch.Tensor:
    """Reward for staying alive."""
    return torch.ones(env.num_envs, device=env.device)


class ActionSmoothnessPenalty(ManagerTermBase):
    """
    A reward term for penalizing large instantaneous changes in the network action output.
    This penalty encourages smoother actions over time.
    """

    def __init__(self, cfg: RewardTermCfg, env: ManagerBasedRLEnv):
        """Initialize the term.

        Args:
            cfg: The configuration of the reward term.
            env: The RL environment instance.
        """
        super().__init__(cfg, env)
        self.dt = env.step_dt
        self.prev_prev_action = None
        self.prev_action = None
        # self.__name__ = "action_smoothness_penalty"

    def __call__(self, env: ManagerBasedRLEnv) -> torch.Tensor:
        """Compute the action smoothness penalty.

        Args:
            env: The RL environment instance.

        Returns:
            The penalty value based on the action smoothness.
        """
        # Get the current action from the environment's action manager
        current_action = env.action_manager.action.clone()

        # If this is the first call, initialize the previous actions
        if self.prev_action is None:
            self.prev_action = current_action
            return torch.zeros(current_action.shape[0], device=current_action.device)

        if self.prev_prev_action is None:
            self.prev_prev_action = self.prev_action
            self.prev_action = current_action
            return torch.zeros(current_action.shape[0], device=current_action.device)

        # Compute the smoothness penalty
        penalty = torch.sum(torch.square(current_action - 2 * self.prev_action + self.prev_prev_action), dim=1)

        # Update the previous actions for the next call
        self.prev_prev_action = self.prev_action
        self.prev_action = current_action

        # Apply a condition to ignore penalty during the first few episodes
        startup_env_mask = env.episode_length_buf < 3
        penalty[startup_env_mask] = 0

        # Return the penalty scaled by the configured weight
        return penalty
    

def base_com_height(
    env: ManagerBasedRLEnv,
    target_height: float,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
    sensor_cfg: SceneEntityCfg | None = None,
) -> torch.Tensor:
    """Penalize asset height from its target using L2 squared kernel.

    Note:
        For flat terrain, target height is in the world frame. For rough terrain,
        sensor readings can adjust the target height to account for the terrain.
    """
    # extract the used quantities (to enable type-hinting)
    asset: RigidObject = env.scene[asset_cfg.name]
    if sensor_cfg is not None:
        sensor: RayCaster = env.scene[sensor_cfg.name]
        # Adjust the target height using the sensor data
        adjusted_target_height = target_height + torch.mean(sensor.data.ray_hits_w[..., 2], dim=1)
    else:
        # Use the provided target height directly for flat terrain
        adjusted_target_height = target_height
    # Compute the L2 squared penalty
    return torch.abs(asset.data.root_pos_w[:, 2] - adjusted_target_height)


class ChrisGaitReward(ManagerTermBase):
    def __init__(self, cfg: RewardTermCfg, env: ManagerBasedRLEnv):
        """Initialize the term."""
        super().__init__(cfg, env)
        # Get the sensor name from the config
        sensor_cfg = cfg.params["sensor_cfg"]
        # Get the actual sensor object from the environment and store it
        self.contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]
        # Store the simulation timestep
        self.dt = env.step_dt

    def __call__(
        self,
        env: ManagerBasedRLEnv,
        command_name: str,
        sensor_cfg,
        contact_force_threshold: float
    ) -> torch.Tensor:
        """Compute the reward based on foot contact state."""
        # Use the 'command_name' argument directly
        gait_params = env.command_manager.get_command(command_name)

        # 1. Determine the DESIRED contact state
        desired_contact_states = self._compute_desired_contact_states(gait_params)

        # 2. Determine the ACTUAL contact state
        # Use 'self.contact_sensor' (from init) and 'sensor_cfg' argument
        foot_forces = torch.norm(self.contact_sensor.data.net_forces_w[:, sensor_cfg.body_ids], dim=-1)
        # Use the 'contact_force_threshold' argument directly
        actual_contact_states = (foot_forces > contact_force_threshold).float()

        # 3. Calculate the penalty for any mismatch
        contact_mismatch_penalty = torch.abs(desired_contact_states - actual_contact_states)

        # 4. The final reward is the sum of penalties
        reward = torch.sum(contact_mismatch_penalty, dim=1)

        return reward

    def _compute_desired_contact_states(self, gait_params: torch.Tensor) -> torch.Tensor:
        """Calculate the desired binary contact state (stance=1, swing=0) for each foot.

        Args:
            gait_params: A tensor containing gait parameters [frequency, offsets, durations].

        Returns:
            A tensor where 1.0 indicates a desired stance phase and 0.0 indicates a swing phase.
        """
        # Extract gait parameters
        frequencies = gait_params[:, 0]
        offsets = gait_params[:, 1]
        # The duration parameter defines the fraction of the cycle that should be in the stance phase
        durations = torch.cat(
            [
                gait_params[:, 2].view(self.num_envs, 1),
                gait_params[:, 2].view(self.num_envs, 1),
            ],
            dim=1,
        )

        # Calculate the current position in the gait cycle (from 0.0 to 1.0)
        gait_indices = torch.remainder(self._env.episode_length_buf * self.dt * frequencies, 1.0)

        # Calculate the phase for each foot by applying the specified offsets
        foot_indices = torch.remainder(
            torch.cat(
                [gait_indices.view(self.num_envs, 1), (gait_indices + offsets + 1).view(self.num_envs, 1)],
                dim=1,
            ),
            1.0,
        )

        # A foot is in the desired stance phase if its current phase (foot_index)
        # is less than the stance duration.
        desired_contact_states = (foot_indices < durations).float()

        return desired_contact_states